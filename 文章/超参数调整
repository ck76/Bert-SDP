https://zhuanlan.zhihu.com/p/27905191

超参数调整
在深度神经网络中，调整超参数组合并非易事，因为训练深层神经网络十分耗时，且需要配置多个参数。

接下来，我们简单列举几个影响CNN网络的关键超参数。

学习率

学习率是指在优化算法中更新网络权重的幅度大小。

学习率可以是恒定的、逐渐降低的、基于动量的或者是自适应的，采用哪种学习率取决于所选择优化算法的类型，如SGD、Adam、Adagrad、AdaDelta或RMSProp等算法。

优化策略这方面的内容可参阅量子位之前编译过的“一文看懂各种神经网络优化算法：从梯度下降到Adam方法”。

迭代次数

迭代次数是指整个训练集输入到神经网络进行训练的次数。当测试错误率和训练错误率相差较小时，可认为当前的迭代次数是合适的，否则需继续增大迭代次数，或调整网络结构。

批次大小

在卷积神经网络的学习过程中，小批次会表现得更好，选取范围一般位于区间[16,128]内。

还需要注意的是，CNN网络对批次大小的调整十分敏感。

激活函数

激活函数具有非线性，理论上可以使模型拟合出任何函数。通常情况下，rectifier函数在CNN网络中的效果较好。当然，可以根据实际任务，选择其他类型的激活函数，如Sigmoid和Tanh等等。

隐含层的数目和单元数

增加隐含层数目以加深网络深度，会在一定程度上改善网络性能，但是当测试错误率不再下降时，就需要寻求其他的改良方法。增加隐含层数目也带来一个问题，即提高了训练该网络的计算成本。

当网络的单元数设置过少时，可能会导致欠拟合，而单元数设置过多时，只要采取合适的正则化方式，就不会产生不良影响。

权重初始化

在网络中，通常会使用小随机数来初始化各网络层的权重，以防止产生不活跃的神经元，但是设置过小的随机数可能生成零梯度网络。一般来说，均匀分布方法效果较好。

Dropout方法

作为一种常用的正则化方式，加入Dropout层可以减弱深层神经网络的过拟合效应。该方法会按照所设定的概率参数，在每次训练中随机地不激活一定比例的神经单元。该参数的默认值为0.5。

手动调整超参数是十分费时也不切实际。接下来介绍两种搜索最优超参数的常用方法。

网格搜索和随机搜索

网格搜索是通过穷举法列出不同的参数组合，确定性能最优的结构。随机搜索是从具有特定分布的参数空间中抽取出一定数量的候选组合。

网格搜索方法也需要制定策略，在初始阶段最好先确定各超参数值的大概范围。可以先尝试在较小迭代次数或较小规模的训练集上进行大步幅的网格搜索。然后在下个阶段中，设置更大的迭代次数，或是使用整个训练集，实现小幅精确定位。

虽然在许多机器学习算法中，通常会使用网格搜索来确定超参数组合，但是随着参数量的增大，训练网络所需的计算量呈指数型增长，这种方法在深层神经网络的超参数调整时效果并不是很好。

有研究指出，在深度神经网络的超参数调整中，随机搜索方法比网格搜索的效率更高，具体可参考文末中的“随机搜索在超参数优化中的应用”。

当然，可根据神经网络的理论经验，进行超参数的手动调整在一些场景下也是可行的。

可视化
我们可以通过可视化各个卷积层，来更好地了解CNN网络是如何学习输入图像的特征。

可视化有两种直接方式，分别是可视化激活程度和可视化相关权重。在网络训练过程中，卷积层的激活情况通常会变得更为稀疏和具有局部特性。当不同输入图像的激活图都存在大片未激活的区域，那么可能是设置了过高的学习率使得卷积核不起作用，导致产生零激活图像。

性能优良的神经网络通常含有多个明显而平滑的卷积器，且没有任何干扰特征。若在权重中观察到相关干扰特征，可能原因是网络未被充分训练，或是正则化强度较低导致了过拟合效应。

