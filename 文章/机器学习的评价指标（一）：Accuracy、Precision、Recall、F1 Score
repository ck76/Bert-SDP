https://zhuanlan.zhihu.com/p/364253497

在评价一个二分类的机器学习分类器好坏的时候，我们通常有Accuracy、Precision、Recall、F1 Score等指标可以选择。本文就介绍一下这些指标的定义和使用场景。

一、混淆矩阵
（一）混淆矩阵的介绍

在介绍评价指标之前，我们首先要介绍一下混淆矩阵（confusion matrix）。混淆矩阵本身是对于预测结果的一个粗略评价，可以让我们对预测结果和原始数据有一个宏观的了解。同时我们也会在计算后面的评价指标时用到混淆矩阵中的数。


混淆矩阵里面有四个格子，包含了我们在进行一个二分类预测的时候，预测结果所有可能出现的情况。也就是说，对于任何一个样本进行预测以后，预测结果一定属于这四个格子的其中一个：

左上角True Positive（简写为TP）表示实际上这个样本为Positive，模型也把这个样本预测为Positive的情况。这是我们预测正确的部分。

右下角True Negative（简写为TN）表示实际上这个样本为Negative，模型也把这个样本预测为Negative的情况。这是我们预测正确的部分。

右上角False Positive（简写为FP）表示实际上这个样本为Negative，但是模型预测为了Positive的情况。这是预测错误的部分，也是统计学上的第一类错误（Type I Error）。

左下角False Negative（简写为FN）表示实际上这个样本为Positive，但是模型预测为了Negative的情况。这是预测错误的部分，也是统计学上的第二类错误（Type II Error）。

（二）混淆矩阵的作用

我们可以对于一个二分类模型的预测结果制作一个混淆矩阵。也就是根据数据集中每个样本预测的正确与否，把整个数据集的全部数据，一个一个地分别放到这四个格子的对应位置中，并求出每个格子分别总共有多少个样本。

这样通过这个矩阵，我们只要一眼就能对于原始数据和预测结果有一个大致的了解。具体包括数据集中：

总共有多少数据（四个格子的数字求和）；
预测对的有多少（主对角线求和）；
预测错的有多少（副对角线求和）；
有多少实际上是positive（第一列求和）；
有多少实际上是negative（第二列求和）；
有多少被预测为positive（第一行求和）；
有多少被预测为negative（第二行求和）。
进一步地：

通过4和5的比例关系，我们可以知道这个数据集是否为非平衡的数据集（数据集中某一个标签的样本占了大多数）。
通过6和7的比例关系，我们可以知道模型预测是否受到了非平衡样本的影响。
此外，下面所有的评价指标都会直接或者间接地用TP、TN、FP、FN来计算。

注意，虽然我们可以把positive指定为二分类中的任意一者，但我们一般把更关心的情况指定为positive，比如“用户会流失”、“是垃圾邮件”、“是阳性肿瘤”、“将要发生地震”。

（三）真实示例

下图是实际使用sklearn计算混淆矩阵时得到的输出：


它的正确的阅读方法如下图所示，读者可以按照“（二）混淆矩阵的作用”中介绍的方法阅读一下这个混淆矩阵：


注意，sklearn输出的混淆矩阵的行列位置顺序和之前一张图不一样，不仅对我们之前演示的混淆矩阵进行了转置，而且行和列的顺序也进行的变化。这一顺序符合python的编码顺序，在总的列表中，第0个子列表是实际为False的，第一个子列表为实际为True的。在每一个子列表中，第0个元素是预测为False的，第1个元素是预测为True的。

二、Accuracy
Accuracy，中文为准确率，指的是“预测正确的样本数÷样本数总数”。计算公式为：


准确率的优点是比较简单直观。通常我们在如下情况的时候使用Accuracy：

数据集是平衡的；
我们要向对于机器学习与数据科学不熟悉的人解释我们的模型；
每一类label对我们来说是一样重要的。因为Accuracy同时考虑了Positive samples和Negative samples。
缺点是不适用于不平衡数据，这里我们可以举一个例子。

例：我们要预测当天是否会发生地震。

由于在某一天内不会发生地震的概率可能性接近99.99%，模型会倾向于无脑地把所有的日子全部预测为不会发生地震，这样它的accuracy将会高达99.99%，只有在那0.01%的、会发生地震的日子是预测错的。

这样的模型虽然accuracy非常高，但是没有任何意义。毕竟我们要的就是在这0.01%的日子把地震预测出来，你全部预测成“没有地震”了要你还有什么用。

在这种情况下，accuracy显然是一个非常差的指标，它无法衡量一个模型的真实预测能力。



三、Precision
（一）Precision的定义

Precision，中文为精确率或者精度，指的是在我们预测为True的样本里面，有多少确实为True的。在信息检索领域，precision也被称为“查准率”。其公式为：


如果我们要求precision高，那么我们是在“求精”，也就是我们不指望着把所有的positive samples都找出来，但是我们希望在我们挑出来的人里面，各个都要是“精英”；“被选中的这批人”一定要干净、纯粹，那些“鱼目混珠”的negative sample越少越好。哪怕是有些“潜力股”被我们放走了也在所不惜。

我们可以把高precision的模型看做是一个谨小慎微、小心翼翼的“检察官”，他对“质”的要求很高，哪怕我抓到的人很少也没关系，但只要是我抓出来的人，就一定是犯过事的。他绝不冤枉一个好人。

Precision的分母是我们预测的结果。也就是，我们三下五除二对所有的样本进行预测，交了一份答卷以后，看看我们预测为True的样本里面真的是True的比例有多大，所以叫做precision精确率。简记为“Precision对应着预测”。

（二）什么情况下我们要求Precision要高？

当“把一个实际是False的样本错标为True”的成本很高，但是“把一个实际是True的样本错标为False”成本很低的时候。

也可以简记为：“冤假错案”成本高，“漏网之鱼”成本低。

这里我们举一个例子。

例：判断一封邮件是否是垃圾邮件，是则为True，否则为False。

“冤假错案”成本高：我们知道，邮件沟通相比于微信、短信、电话，一般是用于较为正式的场合，传达了重要的、高价值的信息。这时候，如果我们把一封正常（False）的邮件标记为垃圾邮件（True），这个错误的代价对我们来说太高了，说不定人家几百万的合同、学校的录取通知书、某一产品的注册确认信息就被你标记为垃圾邮件了。

“漏网之鱼”成本低：如果有一些垃圾邮件没有被标记出来，这个的成本是非常低的，大不了就是用户看见了垃圾邮件，多花几秒钟瞟一眼，然后退出来。也就是几秒钟的时间成本。

这时候，我们就要求标记“垃圾邮件”的过程要“求精不求多”，必须一定是“垃圾邮件”才能被我们标记为True，哪怕代价是放过几只漏网之鱼也没关系。因此，在这个案例中，我们要求precision要高。

四、Recall
（一）Recall的定义

Recall，中文是召回率，指的是，实际上为True的样本有多少被我们挑出来了。在信息检索领域，recall也被称为“查全率”。其公式为：


如果我们要求recall高，我们实际上是在求“大而全”，也就是我们希望一定要把所有的positive samples全部找出来，哪怕我们找出来的样本里面有很多“滥竽充数”的negative样本在里面。追求recall高，实际上的原则是，不管你是否真的是positive，只要我看着你“很可疑”，我就管他三七二十一先把你抓出来，哪怕有冤假错案也在所不惜。

我们可以把高recall的模型看做是一个很贪婪的“检察官”，他对“量”要求很高，虽然可能经常抓错人，但是几乎没有犯人能够成功的流窜在外。

Recall的分母是样本的真实值。也就是，当你进行完预测以后，挑选出了一批你预测为True的样本。这时候有一个检查的人，他说“我要看看有多少样本被你漏掉了没预测出来”。你“漏掉的越少”，recall越大。简记为“Recall对应着样本”。

（二）什么情况下我们要求Recall要高？

很简单，就是和precision一节中所说的相反的情况。

当“把一个实际是False的样本错标为True”的成本很低，但是“把一个实际是True的样本错标为False”成本很高的时候。

也可以简记为：“冤假错案”成本低，“漏网之鱼”成本高。

这里我们举一个例子，还是用开头关于地震的例子。

例：判断某一天是否会发生地震，是则为True，否则为False。

“冤假错案”成本低：如果某一天没有地震，但是我们预测为有地震。那么大不了就是所有人都紧急避难到空旷地带，损失了一天之内的某几个小时的时间。可以承受。

“漏网之鱼”成本高：如果某一天有地震，但是我们预测为没有地震，那么所有的人都跟没事人似的在高楼大厦里面待着，那等地震真的发生了的时候，影响为极为重大，代价极为惨重。

这时候，我们就要求预测地震的过程中，recall要高，也就是有地震迹象了，我们尽量预测为positive，即当天会有地震。哪怕是有那么两出“冤假错案”也没事，因为大家都会理解，毕竟真有地震发生了，如果大家没有紧急避难，那代价太大了。

五、Accuracy和Recall的调和指标：F1 Score
看了上面的介绍，我们当然是希望Precision和Recall都要高。但是这两者很多时候是“鱼与熊掌不可兼得”的。这里我们继续用前面关于垃圾邮件的例子做一些极端的假设作为示范。

例如，我们有1000封邮件，其中垃圾邮件有100封，仍然是希望预测出其中的垃圾邮件。

如果我们希望precision高，那么在极端情况下，我们只把最最可能是垃圾邮件的那一封邮件，也就是No.1的那一封挑出来。在这种“千里选一”的情况下，这一封肯定是垃圾邮件，这时候precision高达100%。但是，Recall相应的就会非常低就只有1%。

如果我们希望recall高，那么极端情况下，我们只要无脑把所有的样本都预测为垃圾邮件，那么此时我们的recall就可以高达100%，但是此时precision相应的只有10%。

我们发现，如果仅仅看recall或者precision中的一个，有可能会在不知情的情况下走向极端；而Accuracy又会受到不平衡样本的影响。那有没有一个万能指标，既能兼顾recall和precision两个方面，又不会受到不平衡样本的影响呢？答案是有的。

此时就要用到我们的F-Measure。

F-Measure是Precision和Recall的加权调和平均：


在确定参数α的值的时候，如果我们越关注recall（相比于precision），我们要选择越大的α。例如，F2 score相比于F1 score，赋予了recall两倍的重要性。

当参数α=1时，就是最常见的F1，也即


F1 score综合考虑了precision和recall两方面的因素，做到了对于两者的调和，即：既要“求精”也要“求全”，做到不偏科。使用f1 score作为评价指标，可以避免上述例子中的极端情况出现。

绝大多数情况下，我们可以直接用f1 score来评价和选择模型。但如果在上面提到的“两类错误的成本”差距比较大的时候，也可以结合recall和precision的其中一个做参考。

注意，f1 score衡量的是模型寻找正例的能力，因为Precision和recall都是衡量寻找正例的指标。

下图是根据上文“混淆矩阵的真实示例”计算得到的常见评价指标：


六、其他评价指标
其他评价指标还包括：ROC-AUC、PR-AUC、AP等，具体内容可以阅读本文的续篇：