"C:\Users\Takada Lab\Documents\GitHub\Bert-SDP\venv\Scripts\python.exe" "C:\Users\Takada Lab\Documents\GitHub\Bert-SDP\run.py"
bert_CNN_BiLSTM_Without_Com_squeeze
Loading data...
190it [00:01, 164.23it/s]
182it [00:01, 173.47it/s]
186it [00:01, 163.60it/s]
Time usage: 0:00:03
Model(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): BertLayerNorm()
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): BertLayerNorm()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): BertLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (conv1): Conv2d(1, 1, kernel_size=(5, 5), stride=(1, 1))
  (lstm): LSTM(764, 64, num_layers=2, batch_first=True, dropout=0.01, bidirectional=True)
  (convs): ModuleList(
    (0): Conv2d(1, 512, kernel_size=(5, 768), stride=(1, 1))
    (1): Conv2d(1, 512, kernel_size=(5, 768), stride=(1, 1))
  )
  (dropout): Dropout(p=0.01, inplace=False)
  (fc_cnn): Linear(in_features=128, out_features=2, bias=True)
  (maxpool): MaxPool1d(kernel_size=508, stride=508, padding=0, dilation=1, ceil_mode=False)
)
Epoch [1/3]
0-------2
hhhhh
(tensor([[  101,  7305,  8916,  ...,     0,     0,     0],
        [  101,  7305,  8916,  ...,     0,     0,     0],
        [  101,  7305,  8916,  ..., 18157,   113,   179],
        ...,
        [  101,  7305,  8916,  ...,   134,  1118, 13053],
        [  101,  7305,  8916,  ...,   116,   107,   117],
        [  101,  7305,  8916,  ...,     0,     0,     0]], device='cuda:0'), tensor([ 62,  49, 512,  57, 512, 247, 512,  92, 135, 512, 162, 512,  88, 512,
        255, 512, 127, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 396,
        155, 512,  89, 512,  55, 512, 274, 512, 184, 512,  61, 512, 512, 207,
         37, 512,  82, 158, 512, 512, 398, 512, 378, 332, 512,  55, 263, 512,
        467, 512, 491, 512,  87, 512, 512, 217], device='cuda:0'), tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'))
hhhhh-0:torch.Size([64, 512])
tensor([[  101,  7305,  8916,  ...,     0,     0,     0],
        [  101,  7305,  8916,  ...,     0,     0,     0],
        [  101,  7305,  8916,  ..., 18157,   113,   179],
        ...,
        [  101,  7305,  8916,  ...,   134,  1118, 13053],
        [  101,  7305,  8916,  ...,   116,   107,   117],
        [  101,  7305,  8916,  ...,     0,     0,     0]], device='cuda:0')
hhhhh-1:torch.Size([64])
tensor([ 62,  49, 512,  57, 512, 247, 512,  92, 135, 512, 162, 512,  88, 512,
        255, 512, 127, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 396,
        155, 512,  89, 512,  55, 512, 274, 512, 184, 512,  61, 512, 512, 207,
         37, 512,  82, 158, 512, 512, 398, 512, 378, 332, 512,  55, 263, 512,
        467, 512, 491, 512,  87, 512, 512, 217], device='cuda:0')
hhhhh-2:torch.Size([64, 512])
tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
hhhhh-labels:torch.Size([64])
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1], device='cuda:0')
torch.Size([64, 512, 768])
torch.Size([64, 1, 512, 768])
torch.Size([64, 1, 508, 764])
torch.Size([64, 508, 764])
torch.Size([64, 508, 128])
torch.Size([64, 128, 508])
torch.Size([64, 128, 1])
torch.Size([64, 128])
C:\Users\Takada Lab\Documents\GitHub\Bert-SDP\pytorch_pretrained\optimization.py:275: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\torch\csrc\utils\python_arg_parser.cpp:1420.)
  next_m.mul_(beta1).add_(1 - beta1, grad)
训练集：
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
torch.Size([64, 512, 768])
torch.Size([64, 1, 512, 768])
torch.Size([64, 1, 508, 764])
torch.Size([64, 508, 764])
torch.Size([64, 508, 128])
torch.Size([64, 128, 508])
torch.Size([64, 128, 1])
torch.Size([64, 128])
torch.Size([64, 512, 768])
torch.Size([64, 1, 512, 768])
torch.Size([64, 1, 508, 764])
torch.Size([64, 508, 764])
torch.Size([64, 508, 128])
torch.Size([64, 128, 508])
torch.Size([64, 128, 1])
torch.Size([64, 128])
验证集：
[0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0
 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1
 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0
 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1]
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
Iter:      0,  Train Loss:   0.7,  Train Acc: 50.00%,  Val Loss:   0.7,  Val Acc: 50.00%,  Time: 0:00:05 *
1-------2
torch.Size([64, 512, 768])
torch.Size([64, 1, 512, 768])
torch.Size([64, 1, 508, 764])
torch.Size([64, 508, 764])
torch.Size([64, 508, 128])
torch.Size([64, 128, 508])
torch.Size([64, 128, 1])
torch.Size([64, 128])
训练集：
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1])
tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
torch.Size([64, 512, 768])
torch.Size([64, 1, 512, 768])
torch.Size([64, 1, 508, 764])
torch.Size([64, 508, 764])
torch.Size([64, 508, 128])
torch.Size([64, 128, 508])
torch.Size([64, 128, 1])
torch.Size([64, 128])
torch.Size([64, 512, 768])
torch.Size([64, 1, 512, 768])
torch.Size([64, 1, 508, 764])
torch.Size([64, 508, 764])
torch.Size([64, 508, 128])
torch.Size([64, 128, 508])
torch.Size([64, 128, 1])
torch.Size([64, 128])
验证集：
[0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0
 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1
 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0
 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1]
[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1
 1 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
Iter:      1,  Train Loss:   0.7,  Train Acc: 50.00%,  Val Loss:  0.68,  Val Acc: 53.12%,  Time: 0:00:10 *
Epoch [2/3]
0-------2
torch.Size([64, 512, 768])
torch.Size([64, 1, 512, 768])
torch.Size([64, 1, 508, 764])
torch.Size([64, 508, 764])
torch.Size([64, 508, 128])
torch.Size([64, 128, 508])
torch.Size([64, 128, 1])
torch.Size([64, 128])
训练集：
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
torch.Size([64, 512, 768])
torch.Size([64, 1, 512, 768])
torch.Size([64, 1, 508, 764])
torch.Size([64, 508, 764])
torch.Size([64, 508, 128])
torch.Size([64, 128, 508])
torch.Size([64, 128, 1])
torch.Size([64, 128])
torch.Size([64, 512, 768])
torch.Size([64, 1, 512, 768])
torch.Size([64, 1, 508, 764])
torch.Size([64, 508, 764])
torch.Size([64, 508, 128])
torch.Size([64, 128, 508])
torch.Size([64, 128, 1])
torch.Size([64, 128])
验证集：
[0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0
 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1
 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0
 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1]
[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
Iter:      2,  Train Loss:  0.68,  Train Acc: 50.00%,  Val Loss:  0.68,  Val Acc: 50.78%,  Time: 0:00:15 *
1-------2
torch.Size([64, 512, 768])
torch.Size([64, 1, 512, 768])
torch.Size([64, 1, 508, 764])
torch.Size([64, 508, 764])
torch.Size([64, 508, 128])
torch.Size([64, 128, 508])
torch.Size([64, 128, 1])
torch.Size([64, 128])
训练集：
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1])
tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
torch.Size([64, 512, 768])
torch.Size([64, 1, 512, 768])
torch.Size([64, 1, 508, 764])
torch.Size([64, 508, 764])
torch.Size([64, 508, 128])
torch.Size([64, 128, 508])
torch.Size([64, 128, 1])
torch.Size([64, 128])
torch.Size([64, 512, 768])
torch.Size([64, 1, 512, 768])
torch.Size([64, 1, 508, 764])
torch.Size([64, 508, 764])
torch.Size([64, 508, 128])
torch.Size([64, 128, 508])
torch.Size([64, 128, 1])
torch.Size([64, 128])
验证集：
[0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0
 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1
 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0
 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1]
[1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1
 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1
 0 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 1
 1 0 0 0 0 0 1 1 1 1 0 0 1 1 0 0 1]
Iter:      3,  Train Loss:  0.67,  Train Acc: 51.56%,  Val Loss:  0.67,  Val Acc: 57.03%,  Time: 0:00:20 *
Epoch [3/3]
0-------2
torch.Size([64, 512, 768])
torch.Size([64, 1, 512, 768])
torch.Size([64, 1, 508, 764])
torch.Size([64, 508, 764])
torch.Size([64, 508, 128])
torch.Size([64, 128, 508])
torch.Size([64, 128, 1])
torch.Size([64, 128])
训练集：
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1])
tensor([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1])
torch.Size([64, 512, 768])
torch.Size([64, 1, 512, 768])
torch.Size([64, 1, 508, 764])
torch.Size([64, 508, 764])
torch.Size([64, 508, 128])
torch.Size([64, 128, 508])
torch.Size([64, 128, 1])
torch.Size([64, 128])
torch.Size([64, 512, 768])
torch.Size([64, 1, 512, 768])
torch.Size([64, 1, 508, 764])
torch.Size([64, 508, 764])
torch.Size([64, 508, 128])
torch.Size([64, 128, 508])
torch.Size([64, 128, 1])
torch.Size([64, 128])
验证集：
[0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0
 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1
 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0
 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1]
[0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 0 0 1 0 1 0 1 0 0 1 1 1
 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 1 1 0 1 0 1 1 1 0 1
 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1
 0 0 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1]
Iter:      4,  Train Loss:  0.65,  Train Acc: 68.75%,  Val Loss:  0.66,  Val Acc: 64.06%,  Time: 0:00:25 *
1-------2
torch.Size([64, 512, 768])
torch.Size([64, 1, 512, 768])
torch.Size([64, 1, 508, 764])
torch.Size([64, 508, 764])
torch.Size([64, 508, 128])
torch.Size([64, 128, 508])
torch.Size([64, 128, 1])
torch.Size([64, 128])
训练集：
tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1])
tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1])
torch.Size([64, 512, 768])
torch.Size([64, 1, 512, 768])
torch.Size([64, 1, 508, 764])
torch.Size([64, 508, 764])
torch.Size([64, 508, 128])
torch.Size([64, 128, 508])
torch.Size([64, 128, 1])
torch.Size([64, 128])
torch.Size([64, 512, 768])
torch.Size([64, 1, 512, 768])
torch.Size([64, 1, 508, 764])
torch.Size([64, 508, 764])
torch.Size([64, 508, 128])
torch.Size([64, 128, 508])
torch.Size([64, 128, 1])
torch.Size([64, 128])
验证集：
[0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0
 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1
 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0
 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1]
[0 1 0 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 0 1 0 0 1 1 1
 1 1 1 0 1 1 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 0 1 1 0 1 0 1 1 1 0 1
 0 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1
 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1]
Iter:      5,  Train Loss:  0.62,  Train Acc: 71.88%,  Val Loss:  0.66,  Val Acc: 64.84%,  Time: 0:00:30 *
torch.Size([64, 512, 768])
torch.Size([64, 1, 512, 768])
torch.Size([64, 1, 508, 764])
torch.Size([64, 508, 764])
torch.Size([64, 508, 128])
torch.Size([64, 128, 508])
torch.Size([64, 128, 1])
torch.Size([64, 128])
torch.Size([64, 512, 768])
torch.Size([64, 1, 512, 768])
torch.Size([64, 1, 508, 764])
torch.Size([64, 508, 764])
torch.Size([64, 508, 128])
torch.Size([64, 128, 508])
torch.Size([64, 128, 1])
torch.Size([64, 128])
验证集：
[0 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1
 1 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 1 0
 1 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1
 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1]
[0 1 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1
 1 0 1 0 0 1 0 1 0 1 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0
 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 1 0 0 0 0 1 1 1
 0 0 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0]
Test Loss:  0.61,  Test Acc: 68.75%
Precision, Recall and F1-Score...
              precision    recall  f1-score   support

       clean     0.7258    0.6618    0.6923        68
       buggy     0.6515    0.7167    0.6825        60

    accuracy                         0.6875       128
   macro avg     0.6887    0.6892    0.6874       128
weighted avg     0.6910    0.6875    0.6877       128

Confusion Matrix...
[[45 23]
 [17 43]]
Time usage: 0:00:03
[W CUDAGuardImpl.h:46] Warning: CUDA warning: driver shutting down (function uncheckedGetDevice)
[W CUDAGuardImpl.h:62] Warning: CUDA warning: driver shutting down (function uncheckedSetDevice)

Process finished with exit code 0
